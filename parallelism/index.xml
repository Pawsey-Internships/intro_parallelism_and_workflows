<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Parallel Computing on Introduction to Parallelism and Supercomputing Workflows</title><link>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/</link><description>Recent content in Parallel Computing on Introduction to Parallelism and Supercomputing Workflows</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 24 Nov 2022 23:35:09 +0800</lastBuildDate><atom:link href="https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/index.xml" rel="self" type="application/rss+xml"/><item><title>Why Parallel Computing?</title><link>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/why-parallel-computing/</link><pubDate>Thu, 24 Nov 2022 23:35:09 +0800</pubDate><guid>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/why-parallel-computing/</guid><description>Why Parallel Computing? In 1965, Intel co-founder Gordon Moore predicted (from 3 data points) that semiconductor density would double every 18 months. ​ ​* Amazingly, his prediction held.​ ​But Moore&amp;rsquo;s law has its limits:​
As device dimensions shrink, controlling the flow of electrical current becomes increasingly tricky. ​
Semiconductor advancement has slowed industry-wide since 2010, below the pace predicted by Moore&amp;rsquo;s law. We also encounter challenges in power efficiency:
A higher CPU clock speed generally requires a higher power draw.</description></item><item><title>Why Not Parallel Computing?</title><link>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/why-not-parallel-computing/</link><pubDate>Thu, 24 Nov 2022 23:35:09 +0800</pubDate><guid>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/why-not-parallel-computing/</guid><description>Parallel algorithms and data processing pipelines usually take more time to implement compared to their serial equivalents.​ They are also often much more complex (and harder to debug).​ Even when understood, they usually require more effort (time) to maintain.​ For one-off tasks, it might be more time efficient just to leave your computer running overnight&amp;hellip;​
…but computational research tasks are rarely one-off.</description></item><item><title>Types of Parallelism.</title><link>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/types-of-parallelism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/types-of-parallelism/</guid><description>There are several types of Parallelism, but as researchers on a virtual machine or supercomputer, we will usually encounter the following:
Data Parallelism: ​
Multiple processes executing the same set of instructions on different &amp;lsquo;chunks&amp;rsquo; of a dataset. Task Parallelism:
Multiple processes executing different instructions on the same or different (but related) sets of data. </description></item><item><title>Concurrency vs Parallelism</title><link>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/concurrency-vs-parallelism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/concurrency-vs-parallelism/</guid><description>Concurrency is progressing on more than one task over a period of time by switching between tasks, e.g.:​
Checking your phone while studying. ​ Taking turns in a game of chess.​ One CPU core switching between instructions from different programs​ No task occurs simultaneously.​
​If the switching occurs fast enough, each process will appear to be progressing simultaneously. As CPUs can process data at a much faster rate than human perception, we can multi-task on a PC with a single CPU core.</description></item><item><title>A few useful terms.</title><link>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/a-few-useful-terms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/a-few-useful-terms/</guid><description>Thread: A subtask in a parallel program or pipeline (different from a CPU thread).​
Shared-Memory Parallelism: A parallel process where threads share the same Memory, which includes variables or data in RAM for CPUs or VRAM for GPUs.
Distributed Parallelism: A parallel process where threads have their own Memory. They do not share the same variables or data.
Synchronisation: Threads come together to access objects or data that other threads have operated on​.</description></item><item><title>Parallel APIs and Frameworks.</title><link>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/parallel-apis-and-frameworks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://Pawsey-Internships.github.io/intro_parallelism_and_workflows/parallelism/parallel-apis-and-frameworks/</guid><description>Some languages support parallelism &amp;lsquo;out of the box&amp;rsquo;:​
Julia (shared-memory and distributed)​ Coarray Fortran (shared-memory and distributed) ​ C# (shared-memory)​ Java (shared-memory)​ …​
There are also many parallel APIs/frameworks:​
OpenCL (shared-memory)​ CUDA (shared-memory)​ OpenMP (shared-memory)​ Message Passing Interface (MPI) (distributed)​ …
If you&amp;rsquo;re not explicitly writing parallel code, you will likely use programs or software libraries built on one of the above APIs/frameworks to take advantage of Parallelism:
Numpy Intel MKL Tensorflow &amp;hellip; What is best depends on the problem at hand and the hardware available.</description></item></channel></rss>