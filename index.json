[{"uri":"/parallelism/","title":"Parallel Computing","tags":[],"description":"","content":"Parallel Computing A type of computation in which many calculations or processors are carried out simultaneously. Larger problems are divided into smaller ones, which can be solved at the same time. Deeply intertwines with High-Performance Computing (HPC). Distinct from concurrent computing. We can speed up important computational tasks 10s, 100s, or 1000s of times with parallel computing.\n\u0026ldquo;The practical difference between obtaining results in hours rather than weeks or years is substantial. It qualitatively changes the range of studies one can conduct. For example, climate change studies, which simulate thousands of Earth years, are feasible only if the time to simulate a year of climate is a few hours.\u0026rdquo; (President\u0026rsquo;s Information Technology Advisory Committee Report, 2005)\n"},{"uri":"/storage/acacia/","title":"Acacia Object Storage.","tags":[],"description":"","content":" /scratch and /software on Setonix do not provide permanent storage space for research data.\nThe data that needs to be stored longer than just a few weeks should be copied to the Acacia object storage service.\nAcacia provides a platform that you can use to store your data as long as your Pawsey project is active.\nThere are two types of accounts available on Acacia, user and project.\nPawsey user accounts come with a 100GB allocation.\nAll active projects with Pawsey supercomputing, Nimbus research cloud, and Visualisation projects have an allocation of at least 1TB.\nAcacia is built upon the open-source software Ceph. Instead of files, your data is stored as objects in buckets:\nBuckets are the basic containers that hold your data.\nEverything you store in Acacia must be contained in a bucket.\nUnlike folders, you cannot nest buckets or put one bucket inside another.\nObjects are the individual pieces of data contained in a bucket.\nAn object is typically a file and any metadata that describes the file.\nObjects cannot be changed while in the storage system.\nTo change an object, you must first download, modify, and then upload it again.\n"},{"uri":"/using-supercomputers-efficiently/data-latency-and-bottlenecks/","title":"Data Latency and Bottleknecks","tags":[],"description":"","content":"Data Latency is the time taken to transfer data to a processor (a core in a CPU or GPU).\nData Location Average Latency Normalised Human Time 3 GHz CPU Clock Cycle 0.3. ns 1 s CPU Cache 0.9-28 ns 2 s - 1 min RAM 70-100 ns 3.5-5.5 min Solid State Disk (NVME) 7-150 µs 2 hrs to 2 days Hard Disk Drive 1-10 µs 11 days to 4 months Another Supercomputing Node 100-1000 ms 3-30 years Depending on the type of work you are doing, the amount of data you are working and the type of system you are working on, your work will be:\nI/O Bound: Progress depends on the speed of input/output operations (e.g. transferring data between the RAM and a disk drive).\nMemory Bound: Compute time depends on the amount of Memory (RAM or VRAM) available to hold working data.\nProcesor Bound: Progress is limited by the speed of processors in the CPU or GPU.\n"},{"uri":"/containers/singularity/","title":"Singularity","tags":[],"description":"","content":" Singularity is a container platform. It targets HPC. A Singularity container built on your laptop should be able to run on most of the world\u0026rsquo;s HPC clusters. Useful links:\nUse with Singularity (Pawsey User Documentation.) Pawsey-Provided Container Images (Pawsey User Documentation.) Singularity Quickstart Guide (External) "},{"uri":"/parallelism/why-parallel-computing/","title":"Why Parallel Computing?","tags":[],"description":"","content":"Why Parallel Computing? In 1965, Intel co-founder Gordon Moore predicted (from 3 data points) that semiconductor density would double every 18 months. ​ ​* Amazingly, his prediction held.​ ​But Moore\u0026rsquo;s law has its limits:​\nAs device dimensions shrink, controlling the flow of electrical current becomes increasingly tricky. ​\nSemiconductor advancement has slowed industry-wide since 2010, below the pace predicted by Moore\u0026rsquo;s law. We also encounter challenges in power efficiency:\nA higher CPU clock speed generally requires a higher power draw.​ CPU power consumption increases at a rate greater than increases in CPU frequency. ​ Solution: Use multiple simpler processors.​\nBy using multiple simpler processes, we can access more computing power without increasing semiconductor density and the (proportional) amount of required electrical power.\n"},{"uri":"/using-supercomputers-efficiently/","title":"Using Supercomputers Efficiently","tags":[],"description":"","content":""},{"uri":"/using-supercomputers-efficiently/case-study-1/","title":"Case Study - Alice","tags":[],"description":"","content":" Alice is working on an astrophysics project that explores how light (photons) are scattered in stellar atmospheres. She is using Monte Carlo Simulation, which simulates the path of 10000s of individual photons based on randomised starting conditions and events. As the path of each simulated independently from the other, this part of her work is embarrassingly parallel. She uses MPI on Setonix to simulate one photon per CPU core. As each of Alice\u0026rsquo;s simulations completes quickly, saving the result of each simulation would result in her work being Memory bound. She solves this by saving her work in \u0026lsquo;chunks\u0026rsquo; of multiple simulation results. "},{"uri":"/storage/getting-started-with-acacia/","title":"Getting Started with Acacia.","tags":[],"description":"","content":" The Acacia User Guide has all of the information you need to get started. A recording of a recent Acacia Object Storage Workshop is available on the Pawsey Youtube Channel. Pawsey supports the minio client on its supercomputing systems. Minio can also be installed on Linux, macOS and Windows. To start using Acacia, you first need to create access keys through [https://portal.pawsey.org.au/]. When you create your key, you will be provided with a command that you can copy and paste to set up access. Creating Buckets:\nmc mb \u0026lt;ALIAS\u0026gt;/\u0026lt;BUCKET_NAME\u0026gt; Delete a bucket:\nmc rb \u0026lt;ALIAS\u0026gt;/\u0026lt;BUCKET_NAME\u0026gt; List Buckets associated with your account or project:\nmc ls \u0026lt;ALIAS\u0026gt; Upload to a Bucket:\nmc cp \u0026lt;SOURCE\u0026gt; \u0026lt;TARGET\u0026gt; Download from a Bucket:\nmc cp \u0026lt;SOURCE\u0026gt; \u0026lt;TARGET\u0026gt; Where \u0026lt;ALIAS\u0026gt; will typically be your project or user name.\n"},{"uri":"/parallelism/why-not-parallel-computing/","title":"Why Not Parallel Computing?","tags":[],"description":"","content":" Parallel algorithms and data processing pipelines usually take more time to implement compared to their serial equivalents.​ They are also often much more complex (and harder to debug).​ Even when understood, they usually require more effort (time) to maintain.​ For one-off tasks, it might be more time efficient just to leave your computer running overnight\u0026hellip;​\n…but computational research tasks are rarely one-off.\n"},{"uri":"/containers/","title":"Containers","tags":[],"description":"","content":" Containers are packages containing all the elements needed to run a process in any computing environment. This allows for the portability of code between different computers, from your laptop to a Nimbus instance or Topaz. They also support the reproducibility of results that depend on specific software versions. Loosely speaking, a container replaces the \u0026lsquo;userland\u0026rsquo; of the host operating system with \u0026lsquo;userland\u0026rsquo; on another operating system - which has access to the devices and computes the power of the host device. There is some performance overhead associated with containers. But for short-to-medium-term projects (like an internship!) time saved troubleshooting is well worth the tradeoff. "},{"uri":"/using-supercomputers-efficiently/case-study-2/","title":"Case Study - Bob","tags":[],"description":"","content":" Bob is working on a computer-vision project that is a machine learning project. He is tasked with training a neural network on a large dataset. Bob is working on Topaz as GPUs excel at performing the mathematical operations used in machine learning through shared-memory Parallelism. Bob finds that his work is Memory bound as he can fit only a small number of images (\u0026lsquo;batch size\u0026rsquo;) in the VRAM of one GPU on Topaz. He solves this by modifying by using the Horovod Python package. It allows him to train his neural network over multiple GPUs, with each GPU learning on a partition of his dataset. Bob\u0026rsquo;s final workflow uses a combination of shared-memory (GPU) and distributed-memory (communication between GPU nodes) parallelism. "},{"uri":"/containers/should-i-use-a-container/","title":"Should I use a container?","tags":[],"description":"","content":"Should I use a container?\nMachine Learning Data Processing Any project with many software dependencies. "},{"uri":"/parallelism/types-of-parallelism/","title":"Types of Parallelism.","tags":[],"description":"","content":"There are several types of Parallelism, but as researchers on a virtual machine or supercomputer, we will usually encounter the following:\nData Parallelism: ​\nMultiple processes executing the same set of instructions on different \u0026lsquo;chunks\u0026rsquo; of a dataset. Task Parallelism:\nMultiple processes executing different instructions on the same or different (but related) sets of data. "},{"uri":"/storage/","title":"Storage","tags":[],"description":"","content":"Storage location on Pawsey systems. Topaz:\n/home - used to store software configuration files that cannot be easily located elsewhere. /scratch - used to store data in use by jobs that are actively queued and running on the supercomputer /group - used to store software and slurm batch scripts Setonix:\n/home - used to store software configuration files that cannot be easily located elsewhere. (1 GB per user) /software - used to store h Pawsey and researcher software installations and Slurm batch scripts. (256 GB per project) /scratch - used to store data in use by jobs that are actively queued and running on the supercomputer. (1 PB limit) Topaz, Setonix, Nimbus and your PC:\nAcacia object storage - High-speed object storage for hosting research data online. It will replace /group on Topaz in the near future.\n/home and /scratch on Topaz may be different than /home and /scratch on Setonix.\nFiles on /scratch are deleted automatically after 30 days from their last access.\nData on /scratch should be moved to Acacia as soon as possible.\n"},{"uri":"/parallelism/concurrency-vs-parallelism/","title":"Concurrency vs Parallelism","tags":[],"description":"","content":"Concurrency is progressing on more than one task over a period of time by switching between tasks, e.g.:​\nChecking your phone while studying. ​ Taking turns in a game of chess.​ One CPU core switching between instructions from different programs​ No task occurs simultaneously.​\n​If the switching occurs fast enough, each process will appear to be progressing simultaneously. As CPUs can process data at a much faster rate than human perception, we can multi-task on a PC with a single CPU core.\nParallelism splits tasks into smaller subtasks that progress independently:​\nWrite the introduction for your thesis while your numerical simulations are running.​ Heats in a chess tournament ​ Two CPUs searching through two halves of one data set to find a maximum value. ​ Unlike Concurrency:\nParallelism can reduce the time it tasks to complete a task.​\nHowever, parallel processes often need to \u0026lsquo;come together to share information.\nWho will proceed to the next heat in the tournament?​ Which CPU has found the actual maximum? "},{"uri":"/parallelism/a-few-useful-terms/","title":"A few useful terms.","tags":[],"description":"","content":"Thread:\tA subtask in a parallel program or pipeline (different from a CPU thread).​\nShared-Memory Parallelism: A parallel process where threads share the same Memory, which includes variables or data in RAM for CPUs or VRAM for GPUs.\nDistributed Parallelism: A parallel process where threads have their own Memory. They do not share the same variables or data.\nSynchronisation:\tThreads come together to access objects or data that other threads have operated on​. In a program, often, these are shared variables. (e.g. a running total)​.\nRace condition​:\tUnexpected results or behaviour due to a fault in the \u0026lsquo;parallel logic of a program. For example, a thread makes a decision based on a shared variable that all threads have not updated. Race conditions arise due to a lack of, or improper, synchronisation.​\nParallel slowdown​:\tNot all parallelisation results in speedup.​ There is an overhead in both the initialisation of threads and, often, communication between them.​ As the number of threads increases, more time is generally spent on inter-thread communication.\nParallel computational tasks can be broady classed as either fine-grained, coarse-grained or embarrassing: ​\n​Fine-grained: Threads frequently communicate (many times a second)​.\nCoarse-Grained:\tThreads infrequently communicate (a few times per second)​.\nEmbarrassing:\tThreads communicate rarely or not at all.\n​In an ideal world, all of our tasks would be embarrassing…but this is generally not the case.\n"},{"uri":"/parallelism/parallel-apis-and-frameworks/","title":"Parallel APIs and Frameworks.","tags":[],"description":"","content":"Some languages support parallelism \u0026lsquo;out of the box\u0026rsquo;:​\nJulia (shared-memory and distributed)​ Coarray Fortran (shared-memory and distributed) ​ C# (shared-memory)​ Java (shared-memory)​ …​\nThere are also many parallel APIs/frameworks:​\nOpenCL (shared-memory)​ CUDA (shared-memory)​ OpenMP (shared-memory)​ Message Passing Interface (MPI) (distributed)​ …\nIf you\u0026rsquo;re not explicitly writing parallel code, you will likely use programs or software libraries built on one of the above APIs/frameworks to take advantage of Parallelism:\nNumpy Intel MKL Tensorflow \u0026hellip; What is best depends on the problem at hand and the hardware available.​\n"},{"uri":"/","title":"Introduction to Parallelism and Supercomputing Workflows","tags":[],"description":"","content":"Yay\n"},{"uri":"/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags/","title":"Tags","tags":[],"description":"","content":""}]