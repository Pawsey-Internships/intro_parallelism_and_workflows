<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction to Parallelism and Supercomputing Workflows</title><link>/</link><description>Recent content on Introduction to Parallelism and Supercomputing Workflows</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 24 Nov 2022 23:35:07 +0800</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Acacia Object Storage.</title><link>/storage/acacia/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/storage/acacia/</guid><description>/scratch and /software on Setonix do not provide permanent storage space for research data.
The data that needs to be stored longer than just a few weeks should be copied to the Acacia object storage service.
Acacia provides a platform that you can use to store your data as long as your Pawsey project is active.
There are two types of accounts available on Acacia, user and project.
Pawsey user accounts come with a 100GB allocation.</description></item><item><title>Data Latency and Bottleknecks</title><link>/using-supercomputers-efficiently/data-latency-and-bottlenecks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/using-supercomputers-efficiently/data-latency-and-bottlenecks/</guid><description>Data Latency is the time taken to transfer data to a processor (a core in a CPU or GPU).
Data Location Average Latency Normalised Human Time 3 GHz CPU Clock Cycle 0.3. ns 1 s CPU Cache 0.9-28 ns 2 s - 1 min RAM 70-100 ns 3.5-5.5 min Solid State Disk (NVME) 7-150 µs 2 hrs to 2 days Hard Disk Drive 1-10 µs 11 days to 4 months Another Supercomputing Node 100-1000 ms 3-30 years Depending on the type of work you are doing, the amount of data you are working and the type of system you are working on, your work will be:</description></item><item><title>Singularity</title><link>/containers/singularity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/containers/singularity/</guid><description> Singularity is a container platform. It targets HPC. A Singularity container built on your laptop should be able to run on most of the world&amp;rsquo;s HPC clusters. Useful links:
Use with Singularity (Pawsey User Documentation.) Pawsey-Provided Container Images (Pawsey User Documentation.) Singularity Quickstart Guide (External)</description></item><item><title>Why Parallel Computing?</title><link>/parallelism/why-parallel-computing/</link><pubDate>Thu, 24 Nov 2022 23:35:09 +0800</pubDate><guid>/parallelism/why-parallel-computing/</guid><description>Why Parallel Computing? In 1965, Intel co-founder Gordon Moore predicted (from 3 data points) that semiconductor density would double every 18 months. ​ ​* Amazingly, his prediction held.​ ​But Moore&amp;rsquo;s law has its limits:​
As device dimensions shrink, controlling the flow of electrical current becomes increasingly tricky. ​
Semiconductor advancement has slowed industry-wide since 2010, below the pace predicted by Moore&amp;rsquo;s law. We also encounter challenges in power efficiency:
A higher CPU clock speed generally requires a higher power draw.</description></item><item><title>Case Study - Alice</title><link>/using-supercomputers-efficiently/case-study-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/using-supercomputers-efficiently/case-study-1/</guid><description>Alice is working on an astrophysics project that explores how light (photons) are scattered in stellar atmospheres. She is using Monte Carlo Simulation, which simulates the path of 10000s of individual photons based on randomised starting conditions and events. As the path of each simulated independently from the other, this part of her work is embarrassingly parallel. She uses MPI on Setonix to simulate one photon per CPU core. As each of Alice&amp;rsquo;s simulations completes quickly, saving the result of each simulation would result in her work being Memory bound.</description></item><item><title>Getting Started with Acacia.</title><link>/storage/getting-started-with-acacia/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/storage/getting-started-with-acacia/</guid><description>The Acacia User Guide has all of the information you need to get started. A recording of a recent Acacia Object Storage Workshop is available on the Pawsey Youtube Channel. Pawsey supports the minio client on its supercomputing systems. Minio can also be installed on Linux, macOS and Windows. To start using Acacia, you first need to create access keys through [https://portal.pawsey.org.au/]. When you create your key, you will be provided with a command that you can copy and paste to set up access.</description></item><item><title>Why Not Parallel Computing?</title><link>/parallelism/why-not-parallel-computing/</link><pubDate>Thu, 24 Nov 2022 23:35:09 +0800</pubDate><guid>/parallelism/why-not-parallel-computing/</guid><description>Parallel algorithms and data processing pipelines usually take more time to implement compared to their serial equivalents.​ They are also often much more complex (and harder to debug).​ Even when understood, they usually require more effort (time) to maintain.​ For one-off tasks, it might be more time efficient just to leave your computer running overnight&amp;hellip;​
…but computational research tasks are rarely one-off.</description></item><item><title>Case Study - Bob</title><link>/using-supercomputers-efficiently/case-study-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/using-supercomputers-efficiently/case-study-2/</guid><description>Bob is working on a computer-vision project that is a machine learning project. He is tasked with training a neural network on a large dataset. Bob is working on Topaz as GPUs excel at performing the mathematical operations used in machine learning through shared-memory Parallelism. Bob finds that his work is Memory bound as he can fit only a small number of images (&amp;lsquo;batch size&amp;rsquo;) in the VRAM of one GPU on Topaz.</description></item><item><title>Should I use a container?</title><link>/containers/should-i-use-a-container/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/containers/should-i-use-a-container/</guid><description>Should I use a container?
Machine Learning Data Processing Any project with many software dependencies.</description></item><item><title>Types of Parallelism.</title><link>/parallelism/types-of-parallelism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/parallelism/types-of-parallelism/</guid><description>There are several types of Parallelism, but as researchers on a virtual machine or supercomputer, we will usually encounter the following:
Data Parallelism: ​
Multiple processes executing the same set of instructions on different &amp;lsquo;chunks&amp;rsquo; of a dataset. Task Parallelism:
Multiple processes executing different instructions on the same or different (but related) sets of data. </description></item><item><title>Concurrency vs Parallelism</title><link>/parallelism/concurrency-vs-parallelism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/parallelism/concurrency-vs-parallelism/</guid><description>Concurrency is progressing on more than one task over a period of time by switching between tasks, e.g.:​
Checking your phone while studying. ​ Taking turns in a game of chess.​ One CPU core switching between instructions from different programs​ No task occurs simultaneously.​
​If the switching occurs fast enough, each process will appear to be progressing simultaneously. As CPUs can process data at a much faster rate than human perception, we can multi-task on a PC with a single CPU core.</description></item><item><title>A few useful terms.</title><link>/parallelism/a-few-useful-terms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/parallelism/a-few-useful-terms/</guid><description>Thread: A subtask in a parallel program or pipeline (different from a CPU thread).​
Shared-Memory Parallelism: A parallel process where threads share the same Memory, which includes variables or data in RAM for CPUs or VRAM for GPUs.
Distributed Parallelism: A parallel process where threads have their own Memory. They do not share the same variables or data.
Synchronisation: Threads come together to access objects or data that other threads have operated on​.</description></item><item><title>Parallel APIs and Frameworks.</title><link>/parallelism/parallel-apis-and-frameworks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/parallelism/parallel-apis-and-frameworks/</guid><description>Some languages support parallelism &amp;lsquo;out of the box&amp;rsquo;:​
Julia (shared-memory and distributed)​ Coarray Fortran (shared-memory and distributed) ​ C# (shared-memory)​ Java (shared-memory)​ …​
There are also many parallel APIs/frameworks:​
OpenCL (shared-memory)​ CUDA (shared-memory)​ OpenMP (shared-memory)​ Message Passing Interface (MPI) (distributed)​ …
If you&amp;rsquo;re not explicitly writing parallel code, you will likely use programs or software libraries built on one of the above APIs/frameworks to take advantage of Parallelism:
Numpy Intel MKL Tensorflow &amp;hellip; What is best depends on the problem at hand and the hardware available.</description></item></channel></rss>